For this summary I read the paper called ``Sequence to Sequence Learning with
Neural Networks'' by Sutskever et al~\cite{NIPS2014_5346}.

\section{Content Summary}

The \textbf{motivation} behind the paper is the use a deep neural network (DNN)
in a sequence to sequence task. In the paper they focus on translating English
text to French, but they say that their work can be adapted to any sequence to
sequence work. The problem is difficult because deep neural networks only
operate on fixed size data; That is why they use a multilayered long short-term
memory (LSTM) system to map differently sized inputs and outputs.

The paper \textbf{contributes} a LSTM system that produces good results with
very little optimization.

There has been a lot of \textbf{related work} DNNs, LSTMs and machine
translation, but in this paper they manage to achieve better results easier
than before.

\textbf{Support} for their results is given through good results on a
English-to-French translation dataset.


\section{Analysis}

I think the \textbf{writing} in this paper is very good. It's easy to
understand but still conveys their results well.

The \textbf{motivation} for the paper is also good; DNNs are very powerful, so
adapting them for use in more applications is useful.

The team's \textbf{contributions} are impressive.

\textbf{Evaluating} their contributions, their results are good, and their
conclusions sensible. They mention that simply reversing the input sentence
improves the results greatly, which seems an important finding.

The paper enables \textbf{future work} in finding more applications for DNNs.
